{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NVRFuynn2uyn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# = Import things\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JEKgWuQx3FXo"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,10),\n",
    "        ) #Here we choose 4 layers with 512, 256, 128 and output as 10 neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GapUnXHk3tO",
    "outputId": "3f2dc7dd-9be7-4bf5-e61b-a501b8729857"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Grayscale(), #set to grayscale\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "#train_dataset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "#train_data = DataLoader(train_dataset, batch_size=1024, shuffle=True) #MNIST datasets from torchvision\n",
    "\n",
    "#or by using own datasets\n",
    "train_dataset = datasets.ImageFolder(\"C:\\\\Users\\\\username\\\\...\" transform=transform) #Change the file path\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# Model definition\n",
    "model = MyModel()\n",
    "# Check if CUDA is available, else use CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Loss function definition :\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Optimizer definition\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002) #Turns out Adam optimizer have better result that SGD\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_data, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  #set to 0 gradiant\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward() #Backward propagation\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() \n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_data)}\")\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97IdptB4b2fQ",
    "outputId": "441de7fd-8851-4e33-d80c-e588ea7779b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = datasets.ImageFolder(\"C:\\\\Users\\\\username\\\\...\",transform = transform)#Change the file path\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "# or by using MNIST datasets\n",
    "#testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "#testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "def test_loop(test_dataloader, model, loss_func):\n",
    "    model.eval()\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    with torch.no_grad():\n",
    "        for Im, label in test_dataloader:\n",
    "            Im, label = Im.to(device), label.to(device)\n",
    "            pred = model(Im)\n",
    "            test_loss += loss_func(pred, label).item()\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "test_loop(testloader, model, loss_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Load test data\n",
    "testset = datasets.ImageFolder(\"C:\\\\Users\\\\username\\\\...\",transform = transform)#Change the file path\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# Test the model and visualize predictions\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of test data\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "model.to(torch.device('cpu')) # moving model to cpu\n",
    "\n",
    "# Predict labels\n",
    "model.eval() #set to evaluation mode\n",
    "with torch.no_grad(): #dissable gradients (takes memory)\n",
    "    outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "# Show number and prediction side by side\n",
    "fig = plt.figure(figsize=(25, 4)) # create a figure with a larger size\n",
    "for idx in range(64): # loop over all images in the batch\n",
    "    ax = fig.add_subplot(4, 16, idx+1, xticks=[], yticks=[]) # create a subplot for each image\n",
    "    ax.imshow(images[idx].squeeze(), cmap='gray') # show the image in grayscale\n",
    "    ax.text(0.5, -0.5, f'{labels[idx].item()} ({predicted[idx].item()})', ha='center', color='green' if labels[idx]==predicted[idx] else 'red') # add text annotation with the number and prediction, using green color for correct predictions and red color for wrong predictions\n",
    "plt.show() # show the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rExCkBXPqB4x"
   },
   "outputs": [],
   "source": [
    "#Save to .pth\n",
    "torch.save(model.state_dict(), \"C:\\\\Users\\\\username\\\\...\") #Change the file path"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:gputorch]",
   "language": "python",
   "name": "conda-env-gputorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
